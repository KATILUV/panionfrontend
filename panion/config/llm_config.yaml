# LLM Service Configuration
model: "gpt-4"  # or "gpt-3.5-turbo" for faster/cheaper responses
temperature: 0.7
max_tokens: 1000
api_key: "${OPENAI_API_KEY}"  # Will be loaded from environment variable
timeout: 30  # seconds
retry_attempts: 3
retry_delay: 1  # seconds

# Model-specific settings
gpt4:
  max_tokens: 4000
  temperature: 0.7
  presence_penalty: 0.0
  frequency_penalty: 0.0

gpt35:
  max_tokens: 2000
  temperature: 0.7
  presence_penalty: 0.0
  frequency_penalty: 0.0 